"""
Phase 3: Autonomous Monitoring Service
Self-healing monitoring, automated alerts, and intelligent performance tracking
"""

import asyncio
import json
import logging
import statistics
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Callable, Dict, List, Optional

# Monitoring dependencies with fallbacks
try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

try:
    import aiohttp

    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False


class AlertSeverity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class MetricType(Enum):
    SYSTEM = "system"
    APPLICATION = "application"
    MODEL = "model"
    BUSINESS = "business"


class MonitoringStatus(Enum):
    HEALTHY = "healthy"
    WARNING = "warning"
    CRITICAL = "critical"
    UNKNOWN = "unknown"


@dataclass
class MetricPoint:
    """Single metric data point"""

    timestamp: datetime
    value: float
    labels: Dict[str, str] = field(default_factory=dict)


@dataclass
class Alert:
    """Monitoring alert"""

    id: str
    name: str
    severity: AlertSeverity
    message: str
    metric_name: str
    current_value: float
    threshold: float
    timestamp: datetime
    resolved: bool = False
    resolved_at: Optional[datetime] = None
    auto_resolved: bool = False


@dataclass
class HealthCheck:
    """Health check configuration"""

    name: str
    url: str
    method: str = "GET"
    timeout: int = 10
    interval: int = 30
    expected_status: int = 200
    expected_content: Optional[str] = None
    headers: Dict[str, str] = field(default_factory=dict)


@dataclass
class ModelPerformanceMetrics:
    """Model performance tracking"""

    model_name: str
    accuracy: float
    latency_ms: float
    throughput_rps: float
    error_rate: float
    timestamp: datetime
    prediction_count: int = 0
    success_count: int = 0


class AutonomousMonitoringService:
    """Advanced autonomous monitoring with self-healing capabilities"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
        self.alerts: Dict[str, Alert] = {}
        self.alert_rules: Dict[str, Dict[str, Any]] = {}
        self.health_checks: Dict[str, HealthCheck] = {}
        self.model_metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=100))
        self.monitoring_tasks: List[asyncio.Task] = []
        self.alert_callbacks: List[Callable] = []
        self.auto_healing_enabled = True
        self._monitoring_started = False

        # Monitoring tasks will be started when needed

    async def _start_monitoring_tasks(self):
        """Start background monitoring tasks"""
        if self._monitoring_started:
            return

        try:
            # System metrics monitoring
            self.monitoring_tasks.append(
                asyncio.create_task(self._monitor_system_metrics())
            )

            # Health checks monitoring
            self.monitoring_tasks.append(
                asyncio.create_task(self._monitor_health_checks())
            )

            # Alert processing
            self.monitoring_tasks.append(asyncio.create_task(self._process_alerts()))

            # Auto-healing
            if self.auto_healing_enabled:
                self.monitoring_tasks.append(
                    asyncio.create_task(self._auto_healing_monitor())
                )

            self._monitoring_started = True
            self.logger.info("üîç Started autonomous monitoring tasks")

        except Exception as e:
            self.logger.error(f"Failed to start monitoring tasks: {e}")

    async def initialize_monitoring(self):
        """Initialize monitoring (called when event loop is available)"""
        if not self._monitoring_started:
            await self._start_monitoring_tasks()

    async def _monitor_system_metrics(self):
        """Monitor system performance metrics"""
        while True:
            try:
                if PSUTIL_AVAILABLE:
                    now = datetime.now()

                    # CPU usage
                    cpu_percent = psutil.cpu_percent(interval=1)
                    await self.record_metric(
                        "system.cpu.usage", cpu_percent, {"type": "percentage"}
                    )

                    # Memory usage
                    memory = psutil.virtual_memory()
                    await self.record_metric(
                        "system.memory.usage", memory.percent, {"type": "percentage"}
                    )
                    await self.record_metric(
                        "system.memory.available", memory.available, {"type": "bytes"}
                    )

                    # Disk usage
                    disk = psutil.disk_usage("/")
                    await self.record_metric(
                        "system.disk.usage", disk.percent, {"type": "percentage"}
                    )

                    # Network I/O
                    network = psutil.net_io_counters()
                    await self.record_metric(
                        "system.network.bytes_sent",
                        network.bytes_sent,
                        {"type": "counter"},
                    )
                    await self.record_metric(
                        "system.network.bytes_recv",
                        network.bytes_recv,
                        {"type": "counter"},
                    )

                    # Process count
                    process_count = len(psutil.pids())
                    await self.record_metric(
                        "system.processes.count", process_count, {"type": "gauge"}
                    )

                await asyncio.sleep(30)  # Monitor every 30 seconds

            except Exception as e:
                self.logger.error(f"System metrics monitoring error: {e}")
                await asyncio.sleep(60)

    async def _monitor_health_checks(self):
        """Monitor configured health check endpoints"""
        while True:
            try:
                if AIOHTTP_AVAILABLE and self.health_checks:
                    tasks = []
                    for check_name, health_check in self.health_checks.items():
                        tasks.append(
                            self._perform_health_check(check_name, health_check)
                        )

                    if tasks:
                        await asyncio.gather(*tasks, return_exceptions=True)

                await asyncio.sleep(30)  # Check every 30 seconds

            except Exception as e:
                self.logger.error(f"Health checks monitoring error: {e}")
                await asyncio.sleep(60)

    async def _perform_health_check(self, check_name: str, health_check: HealthCheck):
        """Perform individual health check"""
        try:
            start_time = time.time()

            if AIOHTTP_AVAILABLE:
                timeout = aiohttp.ClientTimeout(total=health_check.timeout)

                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.request(
                        health_check.method,
                        health_check.url,
                        headers=health_check.headers,
                    ) as response:
                        response_time = (
                            time.time() - start_time
                        ) * 1000  # Convert to ms

                        # Record response time
                        await self.record_metric(
                            f"healthcheck.{check_name}.response_time",
                            response_time,
                            {"endpoint": health_check.url},
                        )

                        # Check status code
                        if response.status == health_check.expected_status:
                            await self.record_metric(
                                f"healthcheck.{check_name}.status",
                                1,  # Success
                                {"endpoint": health_check.url},
                            )

                            # Check content if specified
                            if health_check.expected_content:
                                content = await response.text()
                                if health_check.expected_content in content:
                                    await self.record_metric(
                                        f"healthcheck.{check_name}.content_match",
                                        1,  # Match
                                        {"endpoint": health_check.url},
                                    )
                                else:
                                    await self.record_metric(
                                        f"healthcheck.{check_name}.content_match",
                                        0,  # No match
                                        {"endpoint": health_check.url},
                                    )
                                    await self._trigger_alert(
                                        f"health_check_content_{check_name}",
                                        f"Health check content mismatch for {check_name}",
                                        AlertSeverity.MEDIUM,
                                    )
                        else:
                            await self.record_metric(
                                f"healthcheck.{check_name}.status",
                                0,  # Failure
                                {
                                    "endpoint": health_check.url,
                                    "status": str(response.status),
                                },
                            )
                            await self._trigger_alert(
                                f"health_check_failed_{check_name}",
                                f"Health check failed for {check_name}: {response.status}",
                                AlertSeverity.HIGH,
                            )

        except asyncio.TimeoutError:
            await self.record_metric(
                f"healthcheck.{check_name}.status",
                0,  # Timeout
                {"endpoint": health_check.url, "error": "timeout"},
            )
            await self._trigger_alert(
                f"health_check_timeout_{check_name}",
                f"Health check timeout for {check_name}",
                AlertSeverity.HIGH,
            )
        except Exception as e:
            await self.record_metric(
                f"healthcheck.{check_name}.status",
                0,  # Error
                {"endpoint": health_check.url, "error": str(e)},
            )
            self.logger.error(f"Health check error for {check_name}: {e}")

    async def _process_alerts(self):
        """Process and evaluate alert rules"""
        while True:
            try:
                for rule_name, rule_config in self.alert_rules.items():
                    await self._evaluate_alert_rule(rule_name, rule_config)

                await asyncio.sleep(60)  # Evaluate alerts every minute

            except Exception as e:
                self.logger.error(f"Alert processing error: {e}")
                await asyncio.sleep(60)

    async def _evaluate_alert_rule(self, rule_name: str, rule_config: Dict[str, Any]):
        """Evaluate individual alert rule"""
        try:
            metric_name = rule_config["metric"]
            threshold = rule_config["threshold"]
            operator = rule_config.get("operator", "greater_than")
            severity = AlertSeverity(rule_config.get("severity", "medium"))
            window = rule_config.get("window", 300)  # 5 minutes default

            # Get recent metrics
            if metric_name in self.metrics:
                recent_metrics = [
                    point
                    for point in self.metrics[metric_name]
                    if (datetime.now() - point.timestamp).seconds <= window
                ]

                if recent_metrics:
                    values = [point.value for point in recent_metrics]
                    current_value = values[-1]  # Latest value

                    # Evaluate condition
                    triggered = False
                    if operator == "greater_than" and current_value > threshold:
                        triggered = True
                    elif operator == "less_than" and current_value < threshold:
                        triggered = True
                    elif (
                        operator == "equals" and abs(current_value - threshold) < 0.001
                    ):
                        triggered = True

                    alert_id = f"{rule_name}_{metric_name}"

                    if triggered and alert_id not in self.alerts:
                        # Trigger new alert
                        await self._trigger_alert(
                            alert_id,
                            rule_config.get(
                                "message", f"Alert triggered for {metric_name}"
                            ),
                            severity,
                            metric_name,
                            current_value,
                            threshold,
                        )
                    elif (
                        not triggered
                        and alert_id in self.alerts
                        and not self.alerts[alert_id].resolved
                    ):
                        # Auto-resolve alert
                        await self._resolve_alert(alert_id, auto_resolved=True)

        except Exception as e:
            self.logger.error(f"Alert rule evaluation error for {rule_name}: {e}")

    async def _auto_healing_monitor(self):
        """Monitor system health and trigger auto-healing actions"""
        while True:
            try:
                # Check for critical alerts
                critical_alerts = [
                    alert
                    for alert in self.alerts.values()
                    if alert.severity == AlertSeverity.CRITICAL and not alert.resolved
                ]

                for alert in critical_alerts:
                    await self._trigger_auto_healing(alert)

                # Check model performance degradation
                await self._monitor_model_performance_degradation()

                await asyncio.sleep(60)  # Check every minute

            except Exception as e:
                self.logger.error(f"Auto-healing monitor error: {e}")
                await asyncio.sleep(60)

    async def _trigger_auto_healing(self, alert: Alert):
        """Trigger auto-healing actions for critical alerts"""
        try:
            self.logger.warning(f"üîß Triggering auto-healing for alert: {alert.name}")

            # CPU usage auto-healing
            if "cpu.usage" in alert.metric_name and alert.current_value > 90:
                await self._scale_up_resources()

            # Memory usage auto-healing
            elif "memory.usage" in alert.metric_name and alert.current_value > 85:
                await self._cleanup_memory()

            # Health check failure auto-healing
            elif "healthcheck" in alert.metric_name:
                await self._restart_unhealthy_services()

            # Model performance auto-healing
            elif "model.accuracy" in alert.metric_name and alert.current_value < 0.7:
                await self._trigger_model_retraining()

            self.logger.info(f"‚úÖ Auto-healing completed for: {alert.name}")

        except Exception as e:
            self.logger.error(f"Auto-healing failed for {alert.name}: {e}")

    async def _monitor_model_performance_degradation(self):
        """Monitor model performance for degradation"""
        try:
            for model_name, metrics in self.model_metrics.items():
                if len(metrics) >= 10:  # Need at least 10 data points
                    recent_metrics = list(metrics)[-10:]
                    accuracy_values = [m.accuracy for m in recent_metrics]
                    latency_values = [m.latency_ms for m in recent_metrics]

                    # Check accuracy trend
                    if len(accuracy_values) >= 5:
                        recent_accuracy = statistics.mean(accuracy_values[-5:])
                        baseline_accuracy = statistics.mean(accuracy_values[:5])

                        if recent_accuracy < baseline_accuracy * 0.95:  # 5% degradation
                            await self._trigger_alert(
                                f"model_accuracy_degradation_{model_name}",
                                f"Model accuracy degraded for {model_name}: {recent_accuracy:.3f}",
                                AlertSeverity.HIGH,
                                f"model.{model_name}.accuracy",
                                recent_accuracy,
                                baseline_accuracy * 0.95,
                            )

                    # Check latency trend
                    if len(latency_values) >= 5:
                        recent_latency = statistics.mean(latency_values[-5:])
                        baseline_latency = statistics.mean(latency_values[:5])

                        if recent_latency > baseline_latency * 1.5:  # 50% increase
                            await self._trigger_alert(
                                f"model_latency_increase_{model_name}",
                                f"Model latency increased for {model_name}: {recent_latency:.1f}ms",
                                AlertSeverity.MEDIUM,
                                f"model.{model_name}.latency",
                                recent_latency,
                                baseline_latency * 1.5,
                            )

        except Exception as e:
            self.logger.error(f"Model performance monitoring error: {e}")

    async def record_metric(
        self, name: str, value: float, labels: Optional[Dict[str, str]] = None
    ):
        """Record a metric data point"""
        try:
            metric_point = MetricPoint(
                timestamp=datetime.now(), value=value, labels=labels or {}
            )

            self.metrics[name].append(metric_point)

            # Also record model-specific metrics
            if name.startswith("model.") and "accuracy" in name:
                model_name = name.split(".")[1]
                # This would typically get more comprehensive metrics
                model_metric = ModelPerformanceMetrics(
                    model_name=model_name,
                    accuracy=value,
                    latency_ms=50.0,  # Mock value
                    throughput_rps=100.0,  # Mock value
                    error_rate=0.01,  # Mock value
                    timestamp=datetime.now(),
                )
                self.model_metrics[model_name].append(model_metric)

        except Exception as e:
            self.logger.error(f"Failed to record metric {name}: {e}")

    async def _trigger_alert(
        self,
        alert_id: str,
        message: str,
        severity: AlertSeverity,
        metric_name: str = "",
        current_value: float = 0,
        threshold: float = 0,
    ):
        """Trigger a new alert"""
        try:
            alert = Alert(
                id=alert_id,
                name=alert_id.replace("_", " ").title(),
                severity=severity,
                message=message,
                metric_name=metric_name,
                current_value=current_value,
                threshold=threshold,
                timestamp=datetime.now(),
            )

            self.alerts[alert_id] = alert

            # Call alert callbacks
            for callback in self.alert_callbacks:
                try:
                    await callback(alert)
                except Exception as e:
                    self.logger.error(f"Alert callback failed: {e}")

            self.logger.warning(f"üö® Alert triggered: {alert.name} - {alert.message}")

        except Exception as e:
            self.logger.error(f"Failed to trigger alert: {e}")

    async def _resolve_alert(self, alert_id: str, auto_resolved: bool = False):
        """Resolve an existing alert"""
        try:
            if alert_id in self.alerts:
                alert = self.alerts[alert_id]
                alert.resolved = True
                alert.resolved_at = datetime.now()
                alert.auto_resolved = auto_resolved

                resolve_type = "Auto-resolved" if auto_resolved else "Resolved"
                self.logger.info(f"‚úÖ {resolve_type} alert: {alert.name}")

        except Exception as e:
            self.logger.error(f"Failed to resolve alert {alert_id}: {e}")

    async def add_health_check(self, name: str, url: str, **kwargs):
        """Add a health check endpoint"""
        try:
            health_check = HealthCheck(name=name, url=url, **kwargs)
            self.health_checks[name] = health_check
            self.logger.info(f"üìã Added health check: {name} -> {url}")

        except Exception as e:
            self.logger.error(f"Failed to add health check {name}: {e}")

    async def add_alert_rule(
        self, rule_name: str, metric: str, threshold: float, **kwargs
    ):
        """Add an alert rule"""
        try:
            rule_config = {"metric": metric, "threshold": threshold, **kwargs}

            self.alert_rules[rule_name] = rule_config
            self.logger.info(f"üìè Added alert rule: {rule_name}")

        except Exception as e:
            self.logger.error(f"Failed to add alert rule {rule_name}: {e}")

    async def add_alert_callback(self, callback: Callable):
        """Add callback function for alert notifications"""
        self.alert_callbacks.append(callback)
        self.logger.info("üìû Added alert callback")

    async def _scale_up_resources(self):
        """Scale up system resources (auto-healing action)"""
        self.logger.info("üìà Auto-healing: Scaling up resources")
        # In a real implementation, this would scale Kubernetes pods or VM instances

    async def _cleanup_memory(self):
        """Cleanup memory usage (auto-healing action)"""
        self.logger.info("üßπ Auto-healing: Cleaning up memory")
        # In a real implementation, this would clear caches, restart processes, etc.

    async def _restart_unhealthy_services(self):
        """Restart unhealthy services (auto-healing action)"""
        self.logger.info("üîÑ Auto-healing: Restarting unhealthy services")
        # In a real implementation, this would restart specific services

    async def _trigger_model_retraining(self):
        """Trigger model retraining (auto-healing action)"""
        self.logger.info("üî¨ Auto-healing: Triggering model retraining")
        # In a real implementation, this would start a retraining pipeline

    async def get_metrics(
        self, metric_name: str, time_range: int = 3600
    ) -> List[Dict[str, Any]]:
        """Get metrics for specified time range"""
        try:
            cutoff_time = datetime.now() - timedelta(seconds=time_range)

            if metric_name in self.metrics:
                filtered_metrics = [
                    {
                        "timestamp": point.timestamp.isoformat(),
                        "value": point.value,
                        "labels": point.labels,
                    }
                    for point in self.metrics[metric_name]
                    if point.timestamp >= cutoff_time
                ]
                return filtered_metrics

            return []

        except Exception as e:
            self.logger.error(f"Failed to get metrics for {metric_name}: {e}")
            return []

    async def get_active_alerts(
        self, severity: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """Get active alerts, optionally filtered by severity"""
        try:
            active_alerts = []

            for alert in self.alerts.values():
                if alert.resolved:
                    continue

                if severity and alert.severity.value != severity:
                    continue

                active_alerts.append(
                    {
                        "id": alert.id,
                        "name": alert.name,
                        "severity": alert.severity.value,
                        "message": alert.message,
                        "metric_name": alert.metric_name,
                        "current_value": alert.current_value,
                        "threshold": alert.threshold,
                        "timestamp": alert.timestamp.isoformat(),
                    }
                )

            return sorted(active_alerts, key=lambda x: x["timestamp"], reverse=True)

        except Exception as e:
            self.logger.error(f"Failed to get active alerts: {e}")
            return []

    async def get_system_overview(self) -> Dict[str, Any]:
        """Get system health overview"""
        try:
            # Get latest system metrics
            latest_metrics = {}
            for metric_name, metric_points in self.metrics.items():
                if metric_points:
                    latest_metrics[metric_name] = metric_points[-1].value

            # Count alerts by severity
            alert_counts = {severity.value: 0 for severity in AlertSeverity}
            for alert in self.alerts.values():
                if not alert.resolved:
                    alert_counts[alert.severity.value] += 1

            # Determine overall status
            if alert_counts["critical"] > 0:
                overall_status = MonitoringStatus.CRITICAL
            elif alert_counts["high"] > 0:
                overall_status = MonitoringStatus.WARNING
            elif sum(alert_counts.values()) > 0:
                overall_status = MonitoringStatus.WARNING
            else:
                overall_status = MonitoringStatus.HEALTHY

            return {
                "status": overall_status.value,
                "timestamp": datetime.now().isoformat(),
                "metrics": latest_metrics,
                "alerts": alert_counts,
                "health_checks": len(self.health_checks),
                "model_count": len(self.model_metrics),
                "monitoring_tasks": len(self.monitoring_tasks),
            }

        except Exception as e:
            self.logger.error(f"Failed to get system overview: {e}")
            return {"status": "unknown", "error": str(e)}

    async def get_service_health(self) -> Dict[str, Any]:
        """Get monitoring service health status"""
        return {
            "service": "autonomous_monitoring",
            "status": "healthy",
            "dependencies": {"psutil": PSUTIL_AVAILABLE, "aiohttp": AIOHTTP_AVAILABLE},
            "active_metrics": len(self.metrics),
            "active_alerts": len([a for a in self.alerts.values() if not a.resolved]),
            "health_checks": len(self.health_checks),
            "alert_rules": len(self.alert_rules),
            "monitoring_tasks": len(self.monitoring_tasks),
            "auto_healing_enabled": self.auto_healing_enabled,
            "timestamp": datetime.now().isoformat(),
        }

    # Compatibility method for verification script
    async def health_check(self) -> Dict[str, Any]:
        """Alias for get_service_health for compatibility"""
        return await self.get_service_health()

    # Additional compatibility methods for verification script
    async def collect_system_metrics(self) -> Dict[str, Any]:
        """Collect current system metrics for verification"""
        try:
            metrics = {}
            if PSUTIL_AVAILABLE:
                metrics = {
                    "cpu_usage": psutil.cpu_percent(),
                    "memory_usage": psutil.virtual_memory().percent,
                    "disk_usage": psutil.disk_usage("/").percent,
                    "process_count": len(psutil.pids()),
                }

            return {
                "status": "success",
                "metrics": metrics,
                "timestamp": datetime.now().isoformat(),
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    async def get_healing_status(self) -> Dict[str, Any]:
        """Get auto-healing system status for verification"""
        return {
            "auto_healing_enabled": self.auto_healing_enabled,
            "monitoring_tasks_active": len(self.monitoring_tasks),
            "active_alerts": len([a for a in self.alerts.values() if not a.resolved]),
            "critical_alerts": len(
                [
                    a
                    for a in self.alerts.values()
                    if not a.resolved and a.severity == AlertSeverity.CRITICAL
                ]
            ),
            "healing_actions_available": [
                "scale_up_resources",
                "cleanup_memory",
                "restart_unhealthy_services",
                "trigger_model_retraining",
            ],
            "status": "operational",
            "timestamp": datetime.now().isoformat(),
        }


# Global service instance
autonomous_monitoring_service = AutonomousMonitoringService()
